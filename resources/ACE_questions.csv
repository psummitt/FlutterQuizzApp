"Your team created two networks (VPC) with non-overlapping ranges in Google Cloud in the same region. The first VPC hosts an encryption service on a GKE cluster with cluster autoscaling enabled. The encryption service provides TCP endpoints to encrypt and decrypt data. The second VPC pt-network hosts a user management system on a single Google Cloud Compute Engine VM. The user management system deals with PII data and needs to invoke the encryption endpoints running on the GKE cluster to encrypt and decrypt data. What should you do to enable the compute engine VM invoke the TCP encryption endpoints while minimizing effort?"

Explanation

Explanation:-While it may be possible to set up the networking to let the compute engine instance in pt-network communicate with pods in the GKE

cluster in multiple ways, we need to look for an option that minimizes effort. Generally speaking, this means using Google Cloud Platform services

directly over setting up the service ourselves.

Create a Kubernetes Service with type: Loadbalancer to expose the encryption endpoints running in the pods. Disable propagating Client IP

Addresses to the pods by setting Services' .spec.externalTrafficPolicy to Cluster. Have the GCE VM invoke the TCP encryption endpoints on the

Kubernetes Service DNS address. is not right.

In GKE, services are used to expose pods to the outside world. There are multiple types of services. The three common types are - NodePort,

ClusterIP, and LoadBalancer (there are two more service types - ExternalName and Headless, which are not relevant in this context). We do not

want to create a Cluster IP as this is not accessible outside the cluster. And we do not want to create NodePort as this results in exposing a port on

each node in the cluster; and as we have multiple replicas, this will result in them trying to open the same port on the nodes which fail. The compute

engine instance in pt-network needs a single point of communication to reach GKE, and you can do this by creating a service of type LoadBalancer.

The LoadBalancer service is given a public IP that is externally accessible.

Ref: https://cloud.google.com/kubernetes-engine/docs/how-to/exposing-apps

externalTrafficPolicy denotes how the service should route external traffic - including public access. Rather than trying to explain, I’ll point you to an

excellent blog that does a great job of answering how this works. https://www.asykim.com/blog/deep-dive-into-kubernetes-external-traffic-policies

Since we have cluster autoscaling enabled, we can have more than 1 node and possibly multiple replicas running on each node. So

externalTrafficPolicy set to Cluster plays well with our requirement.

Finally, we configure the compute engine to use the (externally accessible) address of the load balancer.

So this certainly looks like an option, but is it the best option that minimizes effort? One of the disadvantages of this option is that it exposes the pods

publicly by using a service of type LoadBalancer. We want our compute engine to talk to the pods, but do we want to expose our pods to the whole

world? Maybe not!! Let’s look at the other options to find out if there is something more relevant and secure.

Create a Kubernetes Service with type: NodePort to expose the encryption endpoints running in the pods. Set up a custom proxy in another compute

engine VM in pt-network and configure it to forward the traffic to the Kubernetes Service in the other VPC. Have the GCE VM invoke the TCP

encryption endpoints on the proxy DNS address. is not right.

For reasons explained in the above option, we don’t want to create a service of type NodePort. This service opens up a port on each node for each

replica (pod). If we choose to do this, the compute engine doesn’t have a single point to contact. Instead, it would need to contact the GKE cluster

nodes individually - and that is bound to have issues because we have autoscaling enabled and the nodes may scale up and scale down as per the

scaling requirements. New nodes may have different IP addresses to the previous nodes, so unless the Compute engine is continuously supplied

with the IP addresses of the nodes, it can’t reach them. Moreover, we have multiple replicas, and we might have multiple replicas of the pod on the

same node in which case they all can’t open the same node port - once a node port is opened by one replica (pod), it can’t be used by other replicas

on the same node. So this option can be ruled out without going into the rest of the answer.

Create a Kubernetes Service with type: Loadbalancer to expose the encryption endpoints running in the pods. Configure a Cloud Armour security

policy to allow traffic from GCE VM to the Kubernetes Service. Have the GCE VM invoke the TCP encryption endpoints on the Kubernetes Service

DNS address. is not right.

Creating a service of type LoadBalancer and getting a Compute Engine instance to use the address of the load balancer is fine, but Cloud Armor is

not required. You could use Cloud Armor to set up a whitelist policy to only let traffic through from the compute engine instance, but hang on - this

option says “MIG instances”. We don’t have a managed instance group. The question mentions a single instance but not MIG. If we were to assume

the single instance is part of a MIG, i.e. a MIG with a single instance, this option works too. It is more secure than the first option discussed in the

explanation but at the same time more expensive. Let’s look at the other option to see if it provides a secure yet cost-effective way of achieving the

same.

Create a Kubernetes Service with type: Loadbalancer and the cloud.google.com/load-balancer-type: Internal annotation to expose the encryption

endpoints running in the pods. Peer the two VPCs and have the GCE VM invoke the TCP encryption endpoints on the (Internal) Kubernetes Service

DNS address. is the right answer.

Creating a service of type LoadBalancer and getting a Compute Engine instance to use the address of the load balancer is fine. We covered this

previously in the first option in the explanations section.

Adding the annotation cloud.google.com/load-balancer-type: Internal makes your cluster's services accessible to applications outside of your cluster

that use the same VPC network and are located in the same Google Cloud region. So this improves security by not allowing public access; however,

the compute engine is located in a different VPC so it can’t access.

Ref: https://cloud.google.com/kubernetes-engine/docs/how-to/internal-load-balancing

But peering the VPCs together enables the compute engine to access the load balancer IP. And peering is possible because they do not use

overlapping IP ranges. Peering links up the two VPCs and resources inside the VPCs can communicate with each other as if they were all in a single

VPC. More info about VPC peering: https://cloud.google.com/vpc/docs/vpc-peering

So this option is the right answer. It provides a secure and cost-effective way of achieving our requirements. There are several valid answers, but

this option is better than the others.
Question 2: Incorrect

A production application serving live traffic needs an important update deployed gradually. The application is deployed in a

Managed Instance Group (MIG) in the US-Central region. The application receives millions of requests each minute, and you

want to patch the application while ensuring the number of instances in the Managed Instance Group (MIG) does not decrease.

What should you do?

Explanation

Explanation:-This option is the only one that satisfies our two requirements - deploying gradually and ensuring the available capacity does not

decrease. When maxUnavailable is set to 0, the rolling update can not take existing instances out of service. And when maxSurge is set to 1, we let

the rolling update spin a single additional instance. The rolling update then puts the additional instance into service and takes one of the existing

instances out of service for the upgrade. There is no reduction in capacity at any point in time. And the rolling upgrade upgrades 1 instance at a time,

so we gradually deploy the new version. As an example - if we have 10 instances in service, this combination of setting results in 1 additional

instance put into service (resulting in 11 instances serving traffic), then an older instance taken out of service (resulting in 10 instances serving

traffic) and the upgraded instance put back into service (resulting in 11 instances serving traffic). The rolling upgrade continues updating the

remaining 9 instances one at a time. Finally, when all 10 instances have been upgraded, the additional instance that is spun up is deleted. We still

have 10 instances serving live traffic but now on the new version of code.

Ref: https://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups
Question 3: Incorrect

You have asked your supplier to send you a purchase order and you want to enable them to upload the file to a cloud

storage bucket within the next 4 hours. Your supplier does not have a Google account. You want to follow Google

recommended practices. What should you do?

Explanation

Explanation:-This command correctly creates a signed url that is valid for 4 hours and allows PUT (through the -m flag) operations on the bucket.

The supplier can then use the signed URL to upload a file to this bucket within 4 hours.

Ref: https://cloud.google.com/storage/docs/gsutil/commands/signurl
Question 4: Incorrect

Your company stores customer PII data in Cloud Storage buckets. A subset of this data is regularly imported into a

BigQuery dataset to carry out analytics. You want to make sure the access to this bucket is strictly controlled. Your analytics

team needs read access on the bucket so that they can import data in BigQuery. Your operations team needs read/write access

to both the bucket and BigQuery dataset to add Customer PII data of new customers on an ongoing basis. Your Data Vigilance

officers need Administrator access to the Storage bucket and BigQuery dataset. You want to follow Google recommended

practices. What should you do?

Explanation

Explanation:-For Google Cloud Storage service, Google provides predefined roles roles/owner, roles/editor, roles/viewer that match the access

levels we need.

Similarly, Google provides the roles roles/bigquery.dataViewer, roles/bigquery.dataOwner, roles/bigquery.admin that match the access levels we

need.

We can assign these predefined IAM roles to the respective users. Should Google add/modify permissions for these services in the future, we don't

need to modify the roles above as Google does this for us; and this helps future proof our solution.

Ref: https://cloud.google.com/storage/docs/access-control/iam-roles#primitive-roles-intrinsic

Ref: https://cloud.google.com/bigquery/docs/access-control
Question 5: Incorrect

You are working for a cryptocurrency startup, and you have enabled a link to the company’s Initial Coin Offering (ICO)

whitepaper on the company website – which runs off Google Cloud Storage. Your CTO clicked on this link and got prompted to

save the file to their desktop. The CTO thinks this is a poor user experience and has asked you to identify if it is possible to

render the file directly in the browser for all users. What should you do?

Explanation

Explanation:-Content-Type allows browsers to render the object correctly. If the browser prompts users to save files to their machine, it means the

browser does not see the Content-Type as application/pdf. Setting this would ensure the browser displays PDF files within the browser instead of

popping up a download dialogue.

Ref: https://cloud.google.com/storage/docs/gsutil/addlhelp/WorkingWithObjectMetadata#content-type_1
Question 6: Correct

Your company has deployed a wide range of application across several Google Cloud projects in the organization. You are

a security engineer within the Cloud Security team, and an apprentice has recently joined your team. To gain a better

understanding of your company’s Google cloud estate, the apprentice has asked you to provide them access which lets them

have detailed visibility of all projects in the organization. Your manager has approved the request but has asked you to ensure

the access does not let them edit/write access to any resources. Which IAM roles should you assign to the apprentice?

Explanation

Explanation:-roles/viewer provides permissions to view existing resources or data.

roles/resourcemanager.organizationViewer provides access to view an organization.

With the two roles, the security team can view the organization, including all the projects and folders; as well as view all the resources within the

projects.

Ref: https://cloud.google.com/resource-manager/docs/access-control-org

Ref: https://cloud.google.com/iam/docs/understanding-roles#organization-policy-roles
Question 7: Correct

You’ve deployed a microservice that uses sha1 algorithm with a salt value to has usernames. You deployed this to GKE

cluster using deployment file:

You need to make changes to prevent the salt value from being stored in plain text. You want to follow Google-recommended

practices. What should you do?

Explanation

Ref: https://cloud.google.com/kubernetes-engine/docs/concepts/secret

You can create a secret using kubectl create secret generic passwords --from-literal sha1_hash_app_SALT_VALUE= z0rtkty12$!

And you can then modify the YAML file to reference this secret as shown below.
Question 8: Correct

Your company wants to move all documents from a secure internal NAS drive to a Google Cloud Storage (GCS) bucket.

The data contains personally identifiable information (PII) and sensitive customer information. Your company tax auditors need

access to some of these documents. What security strategy would you recommend on GCS?

Explanation

Explanation:-We start with no explicit access to any of the IAM users, and the bucket ACLs can then control which users can access what objects.

This is the most secure way of ensuring just the people who require access to the bucket are provided with access. We block everyone from

accessing the bucket and explicitly provided access to specific users through ACLs.
Question 9: Incorrect

You have a project using BigQuery. You want to list all BigQuery jobs for that project. You want to set this project as the default for the bq command-line tool. What should you do?

Explanation

https://cloud.google.com/bigquery/docs/reference/bq-cli-reference


https://cloud.google.com/sdk/gcloud/reference/config/set
Question 10: Incorrect

You ran the following commands to create two compute instances.

gcloud compute instances create instance1

gcloud compute instances create instance2

Both compute instances were created in europe-west2-a zone but you want to create them in other zones. Your active gcloud

configuration is as shown below.

$ gcloud config list

[component_manager]

disable_update_check = True

[compute]

gce_metadata_read_timeout_sec = 5

zone = europe-west2-a

[core]

account = gcp-ace-lab-user@gmail.com

disable_usage_reporting = False

project = gcp-ace-lab-266520

[metrics]

environment = devshell

You want to modify the gcloud configuration such that you are prompted for a zone when you execute the create instance

commands above. What should you do?

Explanation

Explanation:-This command uses the correct syntax and correctly unsets the zone in gcloud configuration. The next time gcloud compute instances

create command runs, it knows there is no default zone defined in gcloud configuration and therefore prompts for a zone before the instance can be

created.

Ref: https://cloud.google.com/sdk/gcloud/reference/config/unset
Question 11: Correct

You want to migrate a public NodeJS application, which serves requests over HTTPS, from your on-premises data centre

to Google Cloud Platform. You plan to host it on a fleet of instances behind Managed Instances Group (MIG) in Google Compute

Engine. You need to configure a GCP load balancer to terminate SSL session before passing traffic to the VMs. Which GCP Load

balancer should you use?

Explanation

Explanation:-This option fits all requirements. It can serve public traffic, can terminate SSL at the load balancer and follows google recommended

practices.

• "The backends of a backend service can be either instance groups or network endpoint groups (NEGs), but not a combination of both."

• "An external HTTP(S) load balancer distributes traffic from the internet."

• "The client SSL session terminates at the load balancer."

• "For HTTP traffic, use HTTP Load Balancing instead."

Ref: https://cloud.google.com/load-balancing/docs/https
Question 12: Correct

You want to deploy a cost-sensitive application to Google Cloud Compute Engine. You want the application to be up at all

times, but because of the cost-sensitive nature of the application, you only want to run the application in a single VM instance.

How should you configure the managed instance group?

Explanation

Explanation:-Requirements

1. Since we need the application running at all times, we need a minimum 1 instance.

2. Only a single instance of the VM should run, we need a maximum 1 instance.

3. We want the application running at all times. If the VM crashes due to any underlying hardware failure, we want another instance to be added to

MIG so that application can continue to serve requests. We can achieve this by enabling autoscaling.

The only option that satisfies these three is Enable autoscaling on the Managed Instance Group (MIG) and set minimum instances to 1 and maximum

instances to 1.

Ref: https://cloud.google.com/compute/docs/autoscaler
Question 13: Incorrect

Your company wants to move 200 TB of your website clickstream logs from your on-premise data center to Google Cloud

Platform. These logs need to be retained in GCP for compliance requirements. Your business analysts also want to run

analytics on these logs to understand user click behavior on your website. Which of the below would enable you to meet these

requirements? (Select Two)

Explanation

Explanation:-Google Cloud Platform offers several storage classes in Google Cloud Storage that are suitable for storing/archiving logs at a

reasonable cost.

GCP recommends you use

1. Standard storage class if you need to access objects frequently

2. Nearline storage class if you access infrequently i.e. once a month

3. Coldline storage class if you access even less frequently e.g. once a quarter

4. Archive storage for logs archival.

Ref: https://cloud.google.com/storage/docs/storage-classes

Load logs into Google BigQuery. is the right answer.

By loading logs into Google BigQuery, you can securely run and share analytical insights in your organization with a few clicks. BigQuery’s highspeed

streaming insertion API provides a powerful foundation for real-time analytics, making your latest business data immediately available for

analysis.

Ref: https://cloud.google.com/bigquery#marketing-analytics
Question 14: Correct

You want to create a new role and grant it to the SME team. The new role should provide your SME team BigQuery Job

User and Cloud Bigtable User roles on all projects in the organization. You want to minimize operational overhead. You want to

follow Google recommended practices. How should you create the new role?

Explanation

Explanation:-This correctly creates the role and assigns the role to the group at the organization. When any new users join the team, the only

additional task is to add them to the group. Also, when a new project is created under the organization, no additional human intervention is needed.

Since the role is granted at the organization level, it automatically is granted to all the current and future projects belonging to the organization.
Question 15: Incorrect

You developed a web application that lets users upload and share images. You deployed this application in Google

Compute Engine and you have configured Stackdriver Logging. Your application sometimes times out while uploading large

images, and your application generates relevant error log entries that are ingested to Stackdriver Logging. You would now like

to create alerts based on these metrics. You intend to add more compute resources manually when the number of failures

exceeds a threshold. What should you do in order to alert based on these metrics with minimal effort?

Explanation

Explanation:-Our application adds entries to error logs whenever the application times out during image upload and these logs are ingested to

Stackdriver Logging. Since we already have the required data in logs, we just need to create metrics from this log data in Stackdriver Logging. And

we can then set up an alert based on this metric. We can trigger an alert if the number of occurrences of the relevant error message is greater than a

predefined value. Based on the alert, you can manually add more compute resources.

Ref: https://cloud.google.com/logging
Question 16: Incorrect

You have annual audits every year and you need to provide external auditors access to the last 10 years of audit logs. You

want to minimize the cost and operational overhead while following Google recommended practices. What should you do?

(Select Three)

Explanation

Explanation:-Among all the storage solutions offered by Google Cloud Platform, Cloud storage offers the best pricing for long term storage of logs.

Google Cloud Storage offers several storage classes such as Nearline Storage ($0.01 per GB per Month) Coldline Storage ($0.007 per GB per

Month) and Archive Storage ($0.004 per GB per month) which are significantly cheaper than the storage options covered by the above options

above.

Ref: https://cloud.google.com/storage/pricing

Grant external auditors Storage Object Viewer role on the logs storage bucket. is the right answer.

You can provide external auditors access to the logs in the bucket by granting the Storage Object Viewer role which allows them to read any object

stored in any bucket.

Ref: https://cloud.google.com/storage/docs/access-control/iam

Configure a lifecycle management policy on the logs bucket to delete objects older than 10 years. is the right answer.

You need to archive log files for 10 years but you don't need log files older than 10 years. And since you also want to minimize costs, it is a good idea

to set up a lifecycle management policy on the bucket to delete objects that are older than 10 years. Livecycle management configuration is a set of

rules which apply to current and future objects in the bucket. When an object meets the criteria of one of the rules, Cloud Storage automatically

performs a specified action (delete in this case) on the object.

Ref: https://cloud.google.com/storage/docs/lifecycle
Question 17: Correct

You created an update for your application on App Engine. You want to deploy the update without impacting your users.

You want to be able to roll back as quickly as possible if it fails. What should you do?

Explanation

Explanation:-This option enables you to deploy a new version and send all traffic to the new version. If you realize your updated application is not

working, the rollback is as simple as marking your older version as default. This can all be done in the GCP console with a few clicks.

Ref: https://cloud.google.com/appengine/docs/admin-api/deploying-apps
Question 18: Incorrect

You have a web application deployed as a managed instance group based on an instance template. You modified the

startup script used in the instance template and would like the existing instances to pick up changes from the new startup

scripts. Your web application is currently serving live web traffic. You want to propagate the startup script changes to all

instances in the managed instances group while minimizing effort, minimizing cost and ensuring that the available capacity

does not decrease. What would you do?

Explanation

Explanation:-This option achieves the outcome in the most optimal manner. The replace action is used to replace instances in a managed instance

group. When maxUnavailable is set to 0, the rolling update can not take existing instances out of service. And when maxSurge is set to 1, we let the

rolling update spin a single additional instance. The rolling update then puts the additional instance into service and takes one of the existing

instances out of service for replacement. There is no reduction in capacity at any point in time.

Ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#max-unavailable

Ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#max-surge

Ref: https://cloud.google.com/sdk/gcloud/reference/alpha/compute/instance-groups/managed/rolling-action/replace
Question 19: Incorrect

Your company stores sensitive PII data in a cloud storage bucket. The objects are currently encrypted by Googlemanaged

keys. Your compliance department has asked you to ensure all current and future objects in this bucket are

encrypted by customer-managed encryption keys. You want to minimize effort. What should you do?

Explanation

Explanation:-Changing the bucket encryption to use the Customer-managed key ensures all new objects use this key. Now that bucket encryption

is changed to use the Customer-managed key, rewrite all existing objects using gsutil rewrite results in objects being encrypted by the new

Customer-managed key. This is the only option that satisfies our requirements.

Ref: https://cloud.google.com/storage/docs/encryption/using-customer-managed-keys#add-default-key
Question 20: Correct

You have two compute instances in the same VPC but in different regions. You can SSH from one instance to another

instance using their internal IP address but not their external IP address. What could be the reason for SSH failing on external IP

address?

Explanation

Explanation:-The combination of compute instance network tags and VPC firewall rules can certainly result in SSH traffic being allowed from only

subnets IP range. The firewall rule can be configured to allow SSH traffic from just the VPC range e.g. 10.0.0.0/8. In this scenario, all SSH traffic

from within the VPC is accepted but external SSH traffic is blocked.

Ref: https://cloud.google.com/vpc/docs/using-firewalls
Question 21: Correct

Your company has reserved a monthly budget for your project. You want to be informed automatically of your project spend so that you can take action when you approach the limit. What should you do?

Explanation

https://cloud.google.com/appengine/pricing#spending_limit


https://cloud.google.com/billing/docs/how-to/budgets
Question 22: Correct

Your finance department wants you to create a new billing account and link all development and test Google Cloud Projects

to the new billing account. What should you do?

Explanation

Explanation:-The purpose of the Project Billing Manager is to Link/unlink the project to/from a billing account. It is granted at the organization or

project level. Project Billing Manager role allows a user to attach the project to the billing account, but does not grant any rights over resources.

Project Owners can use this role to allow someone else to manage the billing for the project without granting them resource access.

Billing Account Creator - Use this role for initial billing setup or to allow the creation of additional billing accounts.

Ref: https://cloud.google.com/billing/docs/how-to/billing-access
Question 23: Correct

You recently deployed a new application in Google App Engine to serve production traffic. After analyzing logs for various

user flows, you uncovered several issues in your application code and have developed a fix to address the issues. Parts of

your proposed fix could not be validated in the pre-production environment by your testing team as some of the scenarios can

only be validated by an end-user with access to specific data in your production environment. In the company's weekly Change

Approval Board meeting, concerns were raised that the fix could possibly take down the application. It was unanimously

agreed that while the fix is risky, it is a necessary change to the application. You have been asked to suggest a solution that

minimizes the impact of the change going wrong. You also want to minimize costs. What should you do?

Explanation

Explanation:-This option minimizes the risk to the application while also minimizing the complexity and cost. When you deploy a new version to App

Engine, you can choose not to promote it to serve live traffic. Instead, you could set up traffic splitting to split traffic between the two versions - this

can all be done within Google App Engine. Once you send a small portion of traffic to the new version, you can analyze logs to identify if the fix has

worked as expected. If the fix hasn't worked, you can update your traffic splitting configuration to send all traffic back to the old version. If you are

happy your fix has worked, you can send more traffic to the new version or move all user traffic to the new version and delete the old version.

Ref: https://cloud.google.com/appengine/docs/standard/python/splitting-traffic

Ref: https://cloud.google.com/appengine/docs/standard/python/an-overview-of-app-engine
Question 24: Incorrect

You want to reduce storage costs for infrequently accessed data. The data will still be accessed approximately once a

month and data older than 2 years is no longer needed. What should you do to reduce storage costs? (Select 2)

Explanation

Explanation:-Since you don't need data older than 2 years, deleting such data is the right approach. You can set a lifecycle policy to automatically

delete objects older than 2 years. The policy is valid on current as well as future objects and doesn't need any human intervention.

Ref: https://cloud.google.com/storage/docs/lifecycle

Store infrequently accessed data in a Nearline bucket. is the right answer.

Nearline Storage is a low-cost, highly durable storage service for storing infrequently accessed data. Nearline Storage is ideal for data you plan to

read or modify on average once per month or less.

Ref: https://cloud.google.com/storage/docs/storage-classes#nearline
Question 25: Incorrect

A client of yours has asked you for advice because he is looking for a quick and convenient solution for adding functionalities

to an Application.

Whenever a new customer is created in the Firebase database, he wants to perform a series of welcome activities and a series

of follow-up actions, regardless of the specific function that recorded the new customer record.

Which of the following solutions will you suggest (choose 1)?

Explanation

Any of these environments can host the additional functionalities required. The best solution is with Cloud Function, because you can

handle events in the Firebase Realtime Database with no need to update client code. Cloud Functions may have the full administrative privileges to

ensures that each change to the database is processed individually. Furthermore Cloud Functions are a decoupled economic solution, because of

the pay-as-you go model. Functions handle database events in 2 ways; listening for specifically for only creation, update, or deletion events, or you

can listen for any change of any kind to a path. These Cloud Functions event handlers are supported onWrite(), which triggers when data is created,

updated, or deleted in the Realtime Database. onCreate(), which triggers when new data is created in the Realtime Database. onUpdate(), which

triggers when data is updated in the Realtime Database. onDelete(), which triggers when data is deleted from the Realtime Database. For any

further detail: https://firebase.google.com/docs/database/extend-with-functions
Question 26: Incorrect

Your company owns a mobile game that is popular with users all over the world. The mobile game backend uses Cloud

Spanner to store user state. An overnight job exports user state to a Cloud Storage bucket. The app pushes all time-series

events during the game to a streaming Dataflow service that saves them to Cloud Bigtable. You are debugging an in-game

issue raised by a gamer, and you want to join the user state information with data stored in Bigtable to debug. How can you do

this one-off join efficiently?

Explanation

Explanation:-Big query lets you create tables that reference external data sources such as Bigtable and Cloud Storage. You can then join up these

two tables through user fields and apply appropriate filters. You can achieve the result with minimal configuration using this option.

Ref: https://cloud.google.com/bigquery/external-data-sources
Question 27: Incorrect

In addition to ITIL, there are other enterprise IT process management frameworks. Which other standard might you reference when working on enterprise IT management issues?

Explanation

ISO/IEC 20000 is a service management standard.
Question 28: Skipped

Your organization specializes in helping other companies detect if any pages on their website do not align to the specified

standards. To do this, your company has deployed a custom C++ application in your on-premises data centre that crawls all the

web pages of a customer’s website, compares the headers and template to the expected standard and stores the result before

moving on to another customer’s website. This testing takes a lot of time and has resulted in it missing out on the SLA several

times recently. The application team is aware of the slow processing time and wants to run the application on multiple virtual

machines to split the load, but there is no free space in the data centre. You have been asked to identify if it is possible to

migrate this application to Google cloud, ensuring it can autoscale with minimal changes and reduce the processing time. What

GCP service should you recommend?

Explanation

Explanation:-A managed instance group (MIG) contains identical virtual machine (VM) instances that are based on an instance template. MIGs

support auto-healing, load balancing, autoscaling, and auto-updating. Managed instance groups offer auto-scaling capabilities that let you

automatically add or delete instances from a managed instance group based on increases or decreases in load. Autoscaling helps your apps

gracefully handle traffic increases and reduce costs when the need for resources is lower. Autoscaling works by adding more instances to your

instance group when there is more load (upscaling), and deleting instances when the need for instances is lowered (downscaling).

Ref: https://cloud.google.com/compute/docs/autoscaler/
Question 29: Incorrect

Your company installs and manages several types of IoT devices all over the world. Events range from 50,000 to 500,000

messages a second. You want to identify the best solution for ingesting, transforming, storing and analyzing this data in GCP

platform. What GCP services should you use?

Explanation

Explanation:-Cloud Pub/Sub for ingesting, Cloud Dataflow for transforming, Cloud Bigtable for storing and BigQuery for analyzing the time-series

data. is the right answer.

For ingesting time series data, your best bet is Cloud Pub/Sub.

For processing the data in pipelines, your best bet is Cloud Dataflow.

That leaves us with two remaining options; both have BigQuery for analyzing the data. For storage, it is a choice between Bigtable and Datastore.

Bigtable provides out of the box support for time series data. So using Bigtable for Storage is the right answer.

Ref: https://cloud.google.com/bigtable/docs/schema-design-time-series
Question 30: Correct

Your colleague updated a deployment manager template of a production application serving live traffic. You want to deploy

the update to the live environment later during the night when user traffic is at its lowest. The git diff on the pull request shows

the changes are substantial and you would like to review the intended changes without applying the changes in the live

environment. You want to do this as efficiently and quickly as possible. What should you do?

Explanation

Explanation:-After we have written a configuration file, we can preview the configuration before you create a deployment. Previewing a

configuration lets you see the resources that Deployment Manager would create but does not instantiate any resources. In gcloud command-line,

you use the create sub-command with the --preview flag to preview configuration changes.

Ref: https://cloud.google.com/deployment-manager
Question 31: Incorrect

You deployed a number of services to Google App Engine Standard. The services are designed as microservices with

several interdependencies between them. Most services have few version upgrades but some key services have over 20

version upgrades. You identified an issue with the service pt-createOrder and deployed a new version v3 for this service. You

are confident this works and want this new version to receive all traffic for the service. You want to minimize effort and ensure

the availability of service. What should you do?

Explanation

Explanation:-This command correctly migrates the service pt-createOrder to use version 3 and produces the intended outcome while minimizing

effort and ensuring the availability of service.

Ref: https://cloud.google.com/sdk/gcloud/reference/app/versions/migrate
Question 32: Correct

You have a collection of audio/video files over 80GB each that you need to migrate to Google Cloud Storage. The files are

in your on-premises data center. What migration method can you use to help speed up the transfer process?

Explanation

Explanation:-With cloud storage, Object composition can be used for uploading an object in parallel: you can divide your data into multiple chunks,

upload each chunk to a distinct object in parallel, compose your final object, and delete any temporary source objects. This helps maximize your

bandwidth usage and ensures the file is uploaded as fast as possible.

Ref: https://cloud.google.com/storage/docs/composite-objects#uploads
Question 33: Correct

You want to list all the compute instances in zones us-central1-b and europe-west1-d. Which of the commands below

should you run to retrieve this information?

Explanation

Explanation:-gcloud compute instances list - lists Google Compute Engine instances. The output includes internal as well as external IP addresses.

The filter expression --filter="zone:( us-central1-b europe-west1-d )" is used to filter instances from zones us-central1-b and europe-west1-d.

Ref: https://cloud.google.com/sdk/gcloud/reference/compute/instances/list

Here's a sample output of the command.

$gcloud compute instances list

NAME ZONE MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS

gke-cluster-1-default-pool-8c599c87-16g9 us-central1-a n1-standard-1 10.128.0.8 35.184.212.227 RUNNING

gke-cluster-1-default-pool-8c599c87-36xh us-central1-b n1-standard-1 10.129.0.2 34.68.254.220 RUNNING

gke-cluster-1-default-pool-8c599c87-lprq us-central1-c n1-standard-1 10.130.0.13 35.224.96.151 RUNNING

$gcloud compute instances list --filter="zone:( us-central1-b europe-west1-d )"

NAME ZONE MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS

gke-cluster-1-default-pool-8c599c87-36xhus-central1-bn1-standard-110.129.0.234.68.254.220RUNNING
Question 34: Incorrect

You deployed a Python application to GCP App Engine Standard service in the us-central region. Most of your customers

are based in Japan and are experiencing slowness due to the latency. You want to transfer the application from us-central

region to asia-northeast1 region to minimize latency. What should you do?

Explanation

Explanation:-App Engine is regional, and you cannot change an app's region after you set it. Therefore, the only way to have an app run in another

region is by creating a new project and targeting the app engine to run in the required region (asia-northeast1 in our case).

Ref: https://cloud.google.com/appengine/docs/locations
Question 35: Incorrect

In Cloud Shell, your active gcloud configuration is as shown below.

You want to create two compute instances - one in europe-west2-a and another in europe-west2-b. What should you do? (Select

2)

Explanation

Explanation:-The default compute/zone property is europe-west2-a in the current gcloud configuration so executing the first gcloud compute

instances create command creates the instance in europe-west2-a zone. Next, executing the gcloud config set compute/zone europe-west2-b

changes the default compute/zone property in default configuration to europe-west2-b. Executing the second gcloud compute instances create

command creates a compute instance in europe-west2-b which is what we want.

Ref: https://cloud.google.com/sdk/gcloud/reference/config/set

Ref: https://cloud.google.com/sdk/gcloud/reference/compute/instances/create

gcloud compute instances create instance1

gcloud compute instances create instance2 --zone=europe-west2-b. is the right answer.

The default compute/zone property is europe-west2-a in the current gcloud configuration so executing the first gcloud compute instances create

command creates the instance in europe-west2-a zone. Next, executing the second gcloud compute instances create command with --zone property

creates a compute instance in provided zone i.e. europe-west2-b instead of using the default zone from the current active configuration.

Ref: https://cloud.google.com/sdk/gcloud/reference/compute/instances/create
Question 36: Correct

Your company retains all its audit logs in BigQuery for 10 years. At the annual audit every year, you need to provide the

auditors' access to the audit logs. You want to follow Google recommended practices. What should you do?

Explanation

Explanation:-For external auditors, Google recommends we grant logging.viewer and bigquery.dataViewer roles. Since auditing happens several

times a year to review the organization's audit logs, it is recommended we create a group with these grants and assign the group to auditor user

accounts during the time of the audit.

Ref: https://cloud.google.com/iam/docs/roles-audit-logging#scenario_external_auditors
Question 37: Incorrect

Users of your application are complaining of slowness when loading the application. You realize the slowness is because

the App Engine deployment serving the application is deployed in us-central whereas all users of this application are closest to

europe-west3. You want to change the region of the App Engine application to europe-west3 to minimize latency. What's the

best way to change the App Engine region?

Explanation

Explanation:-App engine is a regional service, which means the infrastructure that runs your app(s) is located in a specific region and is managed

by Google to be redundantly available across all the zones within that region. Once an app engine deployment is created in a region, it can't be

changed. The only way is to create a new project and create an App Engine instance in europe-west3, send all user traffic to this instance and

delete the app engine instance in us-central.

Ref: https://cloud.google.com/appengine/docs/locations
Question 38: Correct

You want to create a Google Cloud Storage regional bucket logs-archive in the Los Angeles region (us-west2). You want to

use Coldline storage class to minimize costs and you want to retain files for 10 years. Which of the following commands should

you run to create this bucket?

Explanation

Explanation:-This command correctly creates a bucket in Los Angeles, uses Coldline storage class and retains objects for 10 years.

Ref: https://cloud.google.com/storage/docs/gsutil/commands/mb
Question 39: Incorrect

The development team has provided you with a Kubernetes Deployment file. You have no infrastructure yet and need to deploy the application. What should you do?


Explanation

You would need gcloud to create a kubernetes cluster. Once the cluster is created you can use kubectl to manage the deployments. Refer GCP documentation - Kubernetes Cluster Tutorial To create a cluster with the gcloud command-line tool, use the gcloud container clusters command: gcloud container clusters create hello-cluster --num-nodes=3 To deploy and manage applications on a GKE cluster, you must communicate with the Kubernetes cluster management system. You typically do this by using the kubectl command-line tool.  Kubernetes represents applications as Pods, which are units that represent a container (or group of tightly-coupled containers). The Pod is the smallest deployable unit in Kubernetes. In this tutorial, each Pod contains only your hello-app container. The kubectl run command below causes Kubernetes to create a Deployment named hello-web on your cluster. The Deployment manages multiple copies of your application, called replicas, and schedules them to run on the individual nodes in your cluster. In this case, the Deployment will be running only one Pod of your application.
Question 40: Incorrect

You are developing a mobile game that uses Cloud Datastore for gaming leaderboards and player profiles. You want to

test an aspect of this solution locally on your Ubuntu workstation which already has Cloud SDK installed. What should you do?

Explanation

Explanation:-The Datastore emulator provides local emulation of the production Datastore environment. You can use the emulator to develop and

test your application locally.

Ref: https://cloud.google.com/datastore/docs/tools/datastore-emulator
Question 41: Incorrect

You have been asked to create a new Kubernetes Cluster on Google Kubernetes Engine that can autoscale the number of

worker nodes as well as pods. What should you do? (Select 2)

Explanation

Explanation:-GKE's cluster autoscaler automatically resizes the number of nodes in a given node pool, based on the demands of your workloads.

You don't need to manually add or remove nodes or over-provision your node pools. Instead, you specify a minimum and maximum size for the node

pool, and the rest is automatic. When demand is high, cluster autoscaler adds nodes to the node pool. When demand is low, cluster autoscaler

scales back down to a minimum size that you designate. This can increase the availability of your workloads when you need it while controlling

costs.

Ref: https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler

Enable Horizontal Pod Autoscaling for the kubernetes deployment. is the right answer.

Horizontal Pod Autoscaler scales up and scales down your Kubernetes workload by automatically increasing or decreasing the number of Pods in

response to the workload's CPU or memory consumption, or in response to custom metrics reported from within Kubernetes or external metrics from

sources outside of your cluster. Horizontal Pod Autoscaling cannot be used for workloads that cannot be scaled, such as DaemonSets.

Ref: https://cloud.google.com/kubernetes-engine/docs/concepts/horizontalpodautoscaler
Question 42: Incorrect

You have two Kubernetes resource configuration files.

deployment.yaml - creates a deployment

service.YAML - sets up a LoadBalancer service to expose the pods.

You don't have a GKE cluster in the development project and you need to provision one. Which of the commands below would

you run in Cloud Shell to create a GKE cluster and deploy the YAML configuration files to create a deployment and service?

Explanation

Explanation:-You create a cluster by running gcloud container clusters create command. You then fetch credentials for a running cluster by running

gcloud container clusters get-credentials command. Finally, you apply the Kubernetes resource configuration by running kubectl apply -f

Ref: https://cloud.google.com/sdk/gcloud/reference/container/clusters/create

Ref: https://cloud.google.com/sdk/gcloud/reference/container/clusters/get-credentials

Ref: https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#apply
Question 43: Correct

You have an app that runs in a VM in GCP Compute Engine.

You have been asked to organize the development / test process to make Automating builds from Source Code.

Which product should you use and with what configuration?

Explanation

The Correct Product is Cloud Build, that is a service that executes your builds on Google Cloud Platform infrastructure. Cloud Build

can import source code from Google Cloud Storage, Cloud Source Repositories, GitHub, or Bitbucket, execute a build to your specifications, and

produce artifacts such as Docker containers or Java archives.

App Engine and GKE may use Cloud Build to realize a continuous integration and continuous deployment solution, so the other answers are wrong

without the Cloud Build product and triggers.

For any further detail: https://cloud.google.com/cloud-build/docs/running-builds/automate-builds
Question 44: Correct

To facilitate disaster recovery, your company wants to save database backup tar files in Cloud Storage bucket. You want to

minimize the cost. Which GCP Cloud Storage class should you use?

Explanation

Explanation:-The ideal answer to this would have been Archive Storage, but that is not one of the options.

Archive Storage is the lowest-cost, highly durable storage service for data archiving, online backup, and disaster recovery. Your data is available

within milliseconds, not hours or days.

https://cloud.google.com/storage/docs/storage-classes#archive

In the absence of Archive Storage Class, the next best option for storing backups is Coldline Storage.

Coldline Storage Class is a very-low-cost, highly durable storage service for storing infrequently accessed data. Coldline Storage is a better choice

than Standard Storage or Nearline Storage in scenarios where slightly lower availability, a 90-day minimum storage duration, and higher costs for

data access are acceptable trade-offs for lowered at-rest storage costs. Coldline Storage is ideal for data you plan to read or modify at most once a

quarter.

Ref: https://cloud.google.com/storage/docs/storage-classes#coldline

Although Nearline, Regional and Multi-Regional can also be used to store the backups, they are expensive in comparison, and Google recommends

we use Coldline for backups.

More information about Nearline: https://cloud.google.com/storage/docs/storage-classes#nearline

More information about Standard/Regional: https://cloud.google.com/storage/docs/storage-classes#standard

More information about Standard/Multi-Regional: https://cloud.google.com/storage/docs/storage-classes#standard
Question 45: Correct

Your company’s new mobile game has gone live, and you have transitioned the backend application to the operations

team. The mobile game uses Cloud Spanner to persist game state, leaderboard and player profile. All operations engineers

require access to view and edit table data to support runtime issues. What should you do?

Explanation

Explanation:-Adding users to a group and granting the role to the group is the right way forward. Also, we assign the role spanner.databaseUser

which allows Read from and write to the Cloud Spanner database; execute SQL queries on the database, including DML and Partitioned DML; and

View and update schema for the database. This option grants the right role to a group and assigns users to the group.
Question 46: Incorrect

You have one GCP project with default region and zone set to us-east1 and us-east1-b respectively. You have another GCP

project with default region and zone set to us-west1 and us-west1-a respectively. You want to provision a VM in each of these

projects efficiently using gcloud CLI. What should you do?

Explanation

Explanation:-Each gcloud configuration has a 1 to 1 relationship with the region (if a region is defined). Since we have two different regions, we

would need to create two separate configurations using gcloud config configurations create

Ref: https://cloud.google.com/sdk/gcloud/reference/config/configurations/create

Secondly, you can activate each configuration independently by running gcloud config configurations activate [NAME]

Ref: https://cloud.google.com/sdk/gcloud/reference/config/configurations/activate

Finally, while each configuration is active, you can run the gcloud compute instances start [NAME] command to start the instance in the

configuration's region.

https://cloud.google.com/sdk/gcloud/reference/compute/instances/start
Question 47: Incorrect

You deployed a Java application in a Google Compute Engine VM that has 3.75 GB Memory and 1 vCPU. At peak usage, the

application experiences java.lang.OutOfMemory errors that take down the application entirely and requires a restart. The CPU

usage at all times is minimal. Your operations team have asked you to increase the memory on the VM instance to 8 GB. You

want to do this while minimizing the cost. What should you do?

Explanation

Explanation:-In Google compute engine, if predefined machine types don't meet your needs, you can create an instance with custom virtualized

hardware settings. Specifically, you can create an instance with a custom number of vCPUs and custom memory, effectively using a custom machine

type. Custom machine types are ideal for the following scenarios:

1. Workloads that aren't a good fit for the predefined machine types that are available to you.

2. Workloads that require more processing power or more memory but don't need all of the upgrades that are provided by the next machine type

level.

In our scenario, we only need a memory upgrade. Moving to a bigger instance would also bump up the CPU which we don't need so we have to use

a custom machine type. It is not possible to change memory while the instance is running, so you need to stop the instance first, change the memory

and then start it again.

Ref: https://cloud.google.com/compute/docs/instances/creating-instance-with-custom-machine-type
Question 48: Incorrect

Your company stores sensitive user information (PII) in three multi-regional buckets in US, Europe and Asia. All three

buckets have data access logging enabled on them. The compliance team has received reports of fraudulent activity and has

begun investigating a customer care representative. It believes the specific individual may have accessed some objects they

are not authorized to and may have added labels to some files in the buckets to enable favourable discounts for their friends.

The compliance team has asked you to provide them with a report of activities for this customer service representative on all

three buckets. How can you do this efficiently?

Explanation

Explanation:-Data access logs are already enabled, so we already record all API calls that read the configuration or metadata of resources, as well

as user-driven API calls that create, modify, or read user-provided resource data. Data Access audit logs do not record the data-access operations

on resources that are publicly shared (available to All Users or All Authenticated Users), or that can be accessed without logging into Google Cloud.

Since we are dealing with sensitive data, it is safe to assume that these buckets are not publicly shared and therefore enabling Data access logging

logs all data-access operations on resources. These logs are sent to Stackdriver where they can be viewed by applying a suitable filter.

Unlike activity logs, retrieving the required information to verify is quicker through Stackdriver as you can apply filters such as

resource.type="gcs_bucket"

(resource.labels.bucket_name="gcp-ace-lab-255520" OR resource.labels.bucket_name="gcp-ace-lab-255521" OR

resource.labels.bucket_name="gcp-ace-lab-255522")

(protoPayload.methodName="storage.objects.get" OR protoPayload.methodName="storage.objects.update")

protoPayload.authenticationInfo.principalEmail="test.gcp.labs.user@gmail.com"

and query just the gets and updates, for specific buckets for a specific user. This option involves fewer steps and is more efficient.

Data access logging is not enabled by default and needs to be enabled explicitly. The screenshot below shows a screenshot for enabling the data

access logging for Google Cloud Storage.
Question 49: Correct

You are designing a mobile game which you hope will be used by numerous users around the world. The game backend

requires a Relational DataBase Management System (RDBMS) for persisting game state and player profiles. You want to select

a database that can scale to a global audience with minimal configuration updates. Which database should you choose?

Explanation

Explanation:-Cloud Spanner is a relational database and is highly scalable. Cloud Spanner is a highly scalable, enterprise-grade, globallydistributed,

and strongly consistent database service built for the cloud specifically to combine the benefits of relational database structure with a

non-relational horizontal scale. This combination delivers high-performance transactions and strong consistency across rows, regions, and

continents with an industry-leading 99.999% availability SLA, no planned downtime, and enterprise-grade security

https://cloud.google.com/spanner
Question 50: Correct

A mission-critical application running on a Managed Instance Group (MIG) in Google Cloud has been having scaling issues.

Although the scaling works, it is not quick enough, and users experience slow response times. The solution architect has

recommended moving to GKE to achieve faster scaling and optimize machine resource utilization. Your colleague

containerized the application and provided you with a Dockerfile. You now need to deploy this in a GKE cluster. How should you

do it?

Explanation

Explanation:-Once you have a docker image, you can push it to the container register. You can then create a deployment YAML file pointing to this

image and use kubectl apply -f to deploy this to the Kubernetes cluster. This command assumes you already have a Kubernetes cluster and you

gcloud environment is set up to talk to this container by executing gcloud container clusters get-credentials --zone=

Ref: https://cloud.google.com/container-registry/docs/pushing-and-pulling

Ref: https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#apply

Ref: https://cloud.google.com/sdk/gcloud/reference/container/clusters/get-credentials
Question 51: Incorrect

Your company runs all its applications in us-central1 region in a single GCP project and single VPC. The company has

recently expanded its operations to Europe, but customers in the EU are complaining about slowness accessing the

application. Your manager has requested you to deploy a new instance in the same project in europe-west1 region to reduce

latency to the EU customers. The newly deployed VM needs to reach a central Citrix Licensing Server in us-central-1. How

should you design the network and firewall rules while adhering to Google Recommended practices?

Explanation

Explanation:-We can create another subnet in the same VPC, and this subnet is located in europe-west1. We can then spin up a new instance in

this subnet. We also have to set up a firewall rule to allow communication between the two subnets. All instances in the two subnets with the same

VPC can communicate through the internal IP Address.

Ref: https://cloud.google.com/vpc
Question 52: Incorrect

Your company has chosen to go serverless to enable developers to focus on writing code without worrying about

infrastructure. You have been asked to identify a GCP Serverless service that does not limit your developers to specific

runtimes. In addition, some of the applications need WebSockets support. What should you suggest?

Explanation

Explanation:-Cloud Run for Anthos leverage Kubernetes and serverless together using Cloud Run integrated with Anthos. As this is containerbased,

we are not limited to specific runtimes. Developers can write code using their favorite languages (Go, Python, Java, C#, PHP, Ruby, Node.js,

Shell, and others). Cloud Run for Anthos is the only serverless GCP offering that supports WebSockets.

https://cloud.google.com/serverless-options
Question 53: Correct

You want to migrate a mission-critical application from the on-premises data centre to Google Cloud Platform. Due to the

mission-critical nature of the application, you want to have 3 idle (unoccupied) instances all the time to ensure the application

always has enough resources to handle sudden bursts in traffic. How should you configure the scaling to meet this

requirement?

Explanation

Explanation:-Automatic scaling creates dynamic instances based on request rate, response latencies, and other application metrics. However, if

you specify the number of minimum idle instances, that specified number of instances run as resident instances while any additional instances are

dynamic.

Ref: https://cloud.google.com/appengine/docs/standard/python/how-instances-are-managed
Question 54: Correct

You are migrating a mission-critical HTTPS Web application from your on-premises data centre to Google Cloud, and you

need to ensure unhealthy compute instances within the autoscaled Managed Instances Group (MIG) are recreated

automatically. What should you do?

Explanation

Explanation:-To improve the availability of your application and to verify that your application is responding, you can configure an auto-healing

policy for your managed instance group (MIG). An auto-healing policy relies on an application-based health check to verify that an application is

responding as expected. If the auto healer determines that an application isn't responding, the managed instance group automatically recreates that

instance. Since our application is an HTTPS web application, we need to set up our health check on port 443, which is the standard port for HTTPS.

Ref: https://cloud.google.com/compute/docs/instance-groups/autohealing-instances-in-migs
Question 55: Incorrect

Your organization is planning the infrastructure for a new large-scale application that will need to store anything between

200 TB to a petabyte of data in NoSQL format for Low-latency read/write and High-throughput analytics. Which storage option

should you use?

Explanation

Explanation:-Cloud Bigtable is a petabyte-scale, fully managed NoSQL database service for large analytical and operational workloads.

Ref: https://cloud.google.com/bigtable/
Question 56: Incorrect

You host a production application in Google Compute Engine in the us-central1-a zone. Your application needs to be

available 24*7 all through the year. The application suffered an outage recently due to a Compute Engine outage in the zone

hosting your application. Your application is also susceptible to slowness during peak usage. You have been asked for a

recommendation on how to modify the infrastructure to implement a cost-effective and scalable solution that can withstand

zone failures. What would you recommend?

Explanation

Explanation:-Distribute your resources across multiple zones and regions to tolerate outages. Google designs zones to be independent of each

other: a zone usually has power, cooling, networking, and control planes that are isolated from other zones, and most single failure events will affect

only a single zone. Thus, if a zone becomes unavailable, you can transfer traffic to another zone in the same region to keep your services running.

Ref: https://cloud.google.com/compute/docs/regions-zones

In addition, a managed instance group (MIG) contains offers auto-scaling capabilities that let you automatically add or delete instances from a

managed instance group based on increases or decreases in load. Autoscaling helps your apps gracefully handle increases in traffic and reduce

costs when the need for resources is lower. Autoscaling works by adding more instances to your instance group when there is more load (upscaling),

and deleting instances when the need for instances is lowered (downscaling).

Ref: https://cloud.google.com/compute/docs/autoscaler
Course content


